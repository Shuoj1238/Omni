<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Base Off-policy Algorithms" href="off_policy.html" /><link rel="prev" title="Lagrangian Methods" href="../saferl/lag.html" />

    <!-- Generated with Sphinx 7.1.2 and Furo 2023.08.19 -->
        <title>Base On-policy Algorithms - </title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="/_/static/css/badge_only.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --color-brand-primary: #4E98C8;
  --color-brand-content: #67A4BA;
  --sd-color-success: #5EA69C;
  --sd-color-info: #76A2DB;
  --sd-color-warning: #AD677E;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style>
<!-- RTD Extra Head -->

<link rel="stylesheet" href="/_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": false, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/docs/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "baserlapi/on_policy", "programming_language": "py", "project": "omnisafe", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "furo", "user_analytics_code": "", "version": "latest"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="/_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand"> </div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text"> </span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../start/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../start/usage.html">Usage Video</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">mathematical theory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../foundations/notations.html">Mathematical Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foundations/vector.html">Vector and Martrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foundations/lagrange.html">Lagrange Duality</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">base rl algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../baserl/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../baserl/ppo.html">Proximal Policy Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">safe rl algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../saferl/cpo.html">Constrained Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferl/pcpo.html">Projection-Based Constrained Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferl/focops.html">First Order Constrained Optimization in Policy Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferl/lag.html">Lagrangian Methods</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">base rl algorithms api</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Base On-policy Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="off_policy.html">Base Off-policy Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_based.html">Base Model-based Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">safe rl algorithms api</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../saferlapi/first_order.html">First Order Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferlapi/second_order.html">Second Order Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferlapi/lagrange.html">Lagrange Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferlapi/penalty_function.html">Penalty Function Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saferlapi/model_based.html">Model-based Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">common api</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/buffer.html">OmniSafe Buffer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/exp_grid.html">OmniSafe Experiment Grid</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/lagrange.html">OmniSafe Lagrange Multiplier</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/normalizer.html">OmniSafe Normalizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/logger.html">OmniSafe Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/simmer_agent.html">OmniSafe Simmer Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/stastics_tool.html">OmniSafe Statistics Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/offline_data.html">OmniSafe Offline Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">utils api</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/config.html">OmniSafe Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/distributed.html">OmniSafe Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/math.html">OmniSafe Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/model.html">OmniSafe Model Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/tools.html">OmniSafe Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">OmniSafe Plotter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">models api</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model/actor.html">OmniSafe Actor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/critic.html">OmniSafe Critic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/actor_critic.html">OmniSafe Actor Critic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/modelbased_model.html">OmniSafe Model-based Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/modelbased_planner.html">OmniSafe Model-based Planner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/offline.html">OmniSafe Offline Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">envs api</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../envs/core.html">OmniSafe Core Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../envs/wrapper.html">OmniSafe Wrapper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../envs/safety_gymnasium.html">Safety Gymnasium Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../envs/mujoco_env.html">Mujoco Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../envs/adapter.html">OmniSafe Adapter</a></li>
</ul>

</div>

<div
  id="furo-sidebar-ad-placement"
  class="flat"
  data-ea-publisher="readthedocs"
  data-ea-type="readthedocs-sidebar"
  data-ea-manual="true"
></div>
</div>

<div id="furo-readthedocs-versions" class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions" tabindex="0">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book">&nbsp;</span>
    v: latest
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      
        <dd><a href="/en/latest/">latest</a></dd>
      
        <dd><a href="/en/stable/">stable</a></dd>
      
        <dd><a href="/en/dev/">dev</a></dd>
      
    </dl>
    <dl>
      <dt>Downloads</dt>
      
    </dl>
    <dl>
      
      <dt>On Read the Docs</dt>
        <dd>
          <a href="//readthedocs.org/projects/omnisafe/?fromdocs=omnisafe">Project Home</a>
        </dd>
        <dd>
          <a href="//readthedocs.org/builds/omnisafe/?fromdocs=omnisafe">Builds</a>
        </dd>
    </dl>
  </div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          

  
  <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/PKU-Alignment/OmniSafe/edit/main/docs/source/baserlapi/on_policy.rst" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="base-on-policy-algorithms">
<h1>Base On-policy Algorithms<a class="headerlink" href="#base-on-policy-algorithms" title="Permalink to this heading">#</a></h1>
<div class="table-wrapper autosummary longtable docutils container">
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient" title="omnisafe.algorithms.on_policy.PolicyGradient"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PolicyGradient</span></code></a>(env_id, cfgs)</p></td>
<td><p>The Policy Gradient algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnisafe.algorithms.on_policy.NaturalPG" title="omnisafe.algorithms.on_policy.NaturalPG"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NaturalPG</span></code></a>(env_id, cfgs)</p></td>
<td><p>The Natural Policy Gradient algorithm.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnisafe.algorithms.on_policy.TRPO" title="omnisafe.algorithms.on_policy.TRPO"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TRPO</span></code></a>(env_id, cfgs)</p></td>
<td><p>The Trust Region Policy Optimization (TRPO) algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnisafe.algorithms.on_policy.PPO" title="omnisafe.algorithms.on_policy.PPO"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PPO</span></code></a>(env_id, cfgs)</p></td>
<td><p>The Proximal Policy Optimization (PPO) algorithm.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="policy-gradient">
<h2>Policy Gradient<a class="headerlink" href="#policy-gradient" title="Permalink to this heading">#</a></h2>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-outline-success sd-rounded-1 docutils">
<div class="sd-card-header sd-bg-success sd-text-white docutils">
<p class="sd-card-text">Documentation</p>
</div>
<div class="sd-card-body docutils">
<dl class="py class">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnisafe.algorithms.on_policy.</span></span><span class="sig-name descname"><span class="pre">PolicyGradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cfgs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">The Policy Gradient algorithm.</p>
<div class="admonition-references admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p class="sd-card-text">Title: Policy Gradient Methods for Reinforcement Learning with Function Approximation</p></li>
<li><p class="sd-card-text">Authors: Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour.</p></li>
<li><p class="sd-card-text">URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/1999/file64d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">PG</a></p></li>
</ul>
</div>
<p class="sd-card-text">Initialize an instance of algorithm.</p>
<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._compute_adv_surrogate">
<span class="sig-name descname"><span class="pre">_compute_adv_surrogate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adv_r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_c</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._compute_adv_surrogate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._compute_adv_surrogate" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Compute surrogate loss.</p>
<p class="sd-card-text">Policy Gradient only use reward advantage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>adv_r</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">reward_advantage</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>adv_c</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">cost_advantage</span></code> sampled from buffer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><strong>The advantage function of reward to update policy network.</strong></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._init">
<span class="sig-name descname"><span class="pre">_init</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._init"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._init" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">The initialization of the algorithm.</p>
<p class="sd-card-text">User can define the initialization of the algorithm by inheriting this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span> <span class="o">=</span> <span class="n">CustomBuffer</span><span class="p">()</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">CustomModel</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._init_env">
<span class="sig-name descname"><span class="pre">_init_env</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._init_env"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._init_env" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Initialize the environment.</p>
<p class="sd-card-text">OmniSafe uses <a class="reference internal" href="../envs/adapter.html#omnisafe.adapter.OnPolicyAdapter" title="omnisafe.adapter.OnPolicyAdapter"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnisafe.adapter.OnPolicyAdapter</span></code></a> to adapt the environment to the
algorithm.</p>
<p class="sd-card-text">User can customize the environment by inheriting this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">_init_env</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="n">CustomAdapter</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><strong>AssertionError</strong> – If the number of steps per epoch is not divisible by the number of
    environments.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._init_log">
<span class="sig-name descname"><span class="pre">_init_log</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._init_log"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._init_log" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Log info about epoch.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Things to log</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">Train/Epoch</p></td>
<td><p class="sd-card-text">Current epoch.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Metrics/EpCost</p></td>
<td><p class="sd-card-text">Average cost of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Metrics/EpRet</p></td>
<td><p class="sd-card-text">Average return of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Metrics/EpLen</p></td>
<td><p class="sd-card-text">Average length of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Values/reward</p></td>
<td><p class="sd-card-text">Average value in <code class="xref py py-meth docutils literal notranslate"><span class="pre">rollout()</span></code> (from critic network) of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Values/cost</p></td>
<td><p class="sd-card-text">Average cost in <code class="xref py py-meth docutils literal notranslate"><span class="pre">rollout()</span></code> (from critic network) of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Values/Adv</p></td>
<td><p class="sd-card-text">Average reward advantage of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Loss/Loss_pi</p></td>
<td><p class="sd-card-text">Loss of the policy network.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Loss/Loss_cost_critic</p></td>
<td><p class="sd-card-text">Loss of the cost critic network.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Train/Entropy</p></td>
<td><p class="sd-card-text">Entropy of the policy network.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Train/StopIters</p></td>
<td><p class="sd-card-text">Number of iterations of the policy network.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Train/PolicyRatio</p></td>
<td><p class="sd-card-text">Ratio of the policy network.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Train/LR</p></td>
<td><p class="sd-card-text">Learning rate of the policy network.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Misc/Seed</p></td>
<td><p class="sd-card-text">Seed of the experiment.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Misc/TotalEnvSteps</p></td>
<td><p class="sd-card-text">Total steps of the experiment.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Time</p></td>
<td><p class="sd-card-text">Total time.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">FPS</p></td>
<td><p class="sd-card-text">Frames per second of the epoch.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._init_model">
<span class="sig-name descname"><span class="pre">_init_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._init_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._init_model" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Initialize the model.</p>
<p class="sd-card-text">OmniSafe uses <a class="reference internal" href="../model/actor_critic.html#omnisafe.models.actor_critic.ConstraintActorCritic" title="omnisafe.models.actor_critic.constraint_actor_critic.ConstraintActorCritic"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnisafe.models.actor_critic.constraint_actor_critic.ConstraintActorCritic</span></code></a>
as the default model.</p>
<p class="sd-card-text">User can customize the model by inheriting this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">_init_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">_actor_critic</span> <span class="o">=</span> <span class="n">CustomActorCritic</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._loss_pi">
<span class="sig-name descname"><span class="pre">_loss_pi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._loss_pi"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._loss_pi" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Computing pi/actor loss.</p>
<p class="sd-card-text">In Policy Gradient, the loss is defined as:</p>
<div class="math-wrapper docutils container" id="equation-baserlapi-on-policy-3">
<div class="math notranslate nohighlight" id="equation-baserlapi-on-policy-3">
<span class="eqno">(4)<a class="headerlink" href="#equation-baserlapi-on-policy-3" title="Permalink to this equation">#</a></span>\[L = -\underset{s_t \sim \rho_{\theta}}{\mathbb{E}} [
    \sum_{t=0}^T ( \frac{\pi^{'}_{\theta}(a_t|s_t)}{\pi_{\theta}(a_t|s_t)} )
     A^{R}_{\pi_{\theta}}(s_t, a_t)
]\]</div>
</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> is the policy network, <span class="math notranslate nohighlight">\(\pi^{'}_{\theta}\)</span>
is the new policy network, <span class="math notranslate nohighlight">\(A^{R}_{\pi_{\theta}}(s_t, a_t)\)</span> is the advantage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">observation</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>act</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">action</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>logp</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">log</span> <span class="pre">probability</span></code> of action sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>adv</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">advantage</span></code> processed. <code class="docutils literal notranslate"><span class="pre">reward_advantage</span></code> here.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><strong>The loss of pi/actor.</strong></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._update">
<span class="sig-name descname"><span class="pre">_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._update" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update actor, critic.
:rtype: <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
<ul class="simple">
<li><p class="sd-card-text">Get the <code class="docutils literal notranslate"><span class="pre">data</span></code> from buffer</p></li>
</ul>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">obs</p></th>
<th class="head"><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">observation</span></code> sampled from buffer.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">act</p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">action</span></code> sampled from buffer.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">target_value_r</p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">reward</span> <span class="pre">value</span></code> sampled from buffer.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">target_value_c</p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">cost</span> <span class="pre">value</span></code> sampled from buffer.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">logp</p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">log</span> <span class="pre">probability</span></code> sampled from buffer.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">adv_r</p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">estimated</span> <span class="pre">advantage</span></code> (e.g. <strong>GAE</strong>) sampled from buffer.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">adv_c</p></td>
<td><p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">estimated</span> <span class="pre">cost</span> <span class="pre">advantage</span></code> (e.g. <strong>GAE</strong>) sampled from buffer.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<ul class="simple">
<li><p class="sd-card-text">Update value net by <a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_reward_critic" title="omnisafe.algorithms.on_policy.PolicyGradient._update_reward_critic"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_update_reward_critic()</span></code></a>.</p></li>
<li><p class="sd-card-text">Update cost net by <a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_cost_critic" title="omnisafe.algorithms.on_policy.PolicyGradient._update_cost_critic"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_update_cost_critic()</span></code></a>.</p></li>
<li><p class="sd-card-text">Update policy net by <a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_actor" title="omnisafe.algorithms.on_policy.PolicyGradient._update_actor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_update_actor()</span></code></a>.</p></li>
</ul>
<p class="sd-card-text">The basic process of each update is as follows:</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Get the data from buffer.</p></li>
<li><p class="sd-card-text">Shuffle the data and split it into mini-batch data.</p></li>
<li><p class="sd-card-text">Get the loss of network.</p></li>
<li><p class="sd-card-text">Update the network by loss.</p></li>
<li><p class="sd-card-text">Repeat steps 2, 3 until the number of mini-batch data is used up.</p></li>
<li><p class="sd-card-text">Repeat steps 2, 3, 4 until the KL divergence violates the limit.</p></li>
</ol>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._update_actor">
<span class="sig-name descname"><span class="pre">_update_actor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_c</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._update_actor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_actor" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update policy network under a double for loop.</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Compute the loss function.</p></li>
<li><p class="sd-card-text">Clip the gradient if <code class="docutils literal notranslate"><span class="pre">use_max_grad_norm</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p class="sd-card-text">Update the network by loss function.</p></li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p class="sd-card-text">For some <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">divergence</span></code> based algorithms (e.g. TRPO, CPO, etc.),
the <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">divergence</span></code> between the old policy and the new policy is calculated.
And the <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">divergence</span></code> is used to determine whether the update is successful.
If the <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">divergence</span></code> is too large, the update will be terminated.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">observation</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>act</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">action</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>logp</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">log_p</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>adv_r</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">reward_advantage</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>adv_c</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">cost_advantage</span></code> sampled from buffer.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._update_cost_critic">
<span class="sig-name descname"><span class="pre">_update_cost_critic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_value_c</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._update_cost_critic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_cost_critic" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update value network under a double for loop.</p>
<p class="sd-card-text">The loss function is <code class="docutils literal notranslate"><span class="pre">MSE</span> <span class="pre">loss</span></code>, which is defined in <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code>.
Specifically, the loss function is defined as:</p>
<div class="math-wrapper docutils container" id="equation-baserlapi-on-policy-4">
<div class="math notranslate nohighlight" id="equation-baserlapi-on-policy-4">
<span class="eqno">(5)<a class="headerlink" href="#equation-baserlapi-on-policy-4" title="Permalink to this equation">#</a></span>\[L = \frac{1}{N} \sum_{i=1}^N (\hat{V} - V)^2\]</div>
</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\hat{V}\)</span> is the predicted cost and <span class="math notranslate nohighlight">\(V\)</span> is the target cost.</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Compute the loss function.</p></li>
<li><p class="sd-card-text">Add the <code class="docutils literal notranslate"><span class="pre">critic</span> <span class="pre">norm</span></code> to the loss function if <code class="docutils literal notranslate"><span class="pre">use_critic_norm</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p class="sd-card-text">Clip the gradient if <code class="docutils literal notranslate"><span class="pre">use_max_grad_norm</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p class="sd-card-text">Update the network by loss function.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">observation</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>target_value_c</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">target_value_c</span></code> sampled from buffer.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient._update_reward_critic">
<span class="sig-name descname"><span class="pre">_update_reward_critic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_value_r</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient._update_reward_critic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_reward_critic" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update value network under a double for loop.</p>
<p class="sd-card-text">The loss function is <code class="docutils literal notranslate"><span class="pre">MSE</span> <span class="pre">loss</span></code>, which is defined in <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code>.
Specifically, the loss function is defined as:</p>
<div class="math-wrapper docutils container" id="equation-baserlapi-on-policy-5">
<div class="math notranslate nohighlight" id="equation-baserlapi-on-policy-5">
<span class="eqno">(6)<a class="headerlink" href="#equation-baserlapi-on-policy-5" title="Permalink to this equation">#</a></span>\[L = \frac{1}{N} \sum_{i=1}^N (\hat{V} - V)^2\]</div>
</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\hat{V}\)</span> is the predicted cost and <span class="math notranslate nohighlight">\(V\)</span> is the target cost.</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Compute the loss function.</p></li>
<li><p class="sd-card-text">Add the <code class="docutils literal notranslate"><span class="pre">critic</span> <span class="pre">norm</span></code> to the loss function if <code class="docutils literal notranslate"><span class="pre">use_critic_norm</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p class="sd-card-text">Clip the gradient if <code class="docutils literal notranslate"><span class="pre">use_max_grad_norm</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p class="sd-card-text">Update the network by loss function.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">observation</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>target_value_r</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">target_value_r</span></code> sampled from buffer.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PolicyGradient.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/policy_gradient.html#PolicyGradient.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PolicyGradient.learn" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">This is main function for algorithm update.</p>
<p class="sd-card-text">It is divided into the following steps:
:rtype: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
<ul class="simple">
<li><p class="sd-card-text"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rollout()</span></code>: collect interactive data from environment.</p></li>
<li><p class="sd-card-text"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code>: perform actor/critic updates.</p></li>
<li><p class="sd-card-text"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code>: epoch/update information for visualization and terminal log print.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>ep_ret</strong> – Average episode return in final epoch.</p></li>
<li><p class="sd-card-text"><strong>ep_cost</strong> – Average episode cost in final epoch.</p></li>
<li><p class="sd-card-text"><strong>ep_len</strong> – Average episode length in final epoch.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</section>
<section id="natural-policy-gradient">
<h2>Natural Policy Gradient<a class="headerlink" href="#natural-policy-gradient" title="Permalink to this heading">#</a></h2>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-outline-success sd-rounded-1 docutils">
<div class="sd-card-header sd-bg-success sd-text-white docutils">
<p class="sd-card-text">Documentation</p>
</div>
<div class="sd-card-body docutils">
<dl class="py class">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.NaturalPG">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnisafe.algorithms.on_policy.</span></span><span class="sig-name descname"><span class="pre">NaturalPG</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cfgs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/natural_pg.html#NaturalPG"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.NaturalPG" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">The Natural Policy Gradient algorithm.</p>
<p class="sd-card-text">The Natural Policy Gradient algorithm is a policy gradient algorithm that uses the
<a class="reference external" href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information matrix</a> to approximate
the Hessian matrix. The Fisher information matrix is the second-order derivative of the KL-divergence.</p>
<div class="admonition-references admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p class="sd-card-text">Title: A Natural Policy Gradient</p></li>
<li><p class="sd-card-text">Author: Sham Kakade.</p></li>
<li><p class="sd-card-text">URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf">Natural PG</a></p></li>
</ul>
</div>
<p class="sd-card-text">Initialize an instance of algorithm.</p>
<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.NaturalPG._fvp">
<span class="sig-name descname"><span class="pre">_fvp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/natural_pg.html#NaturalPG._fvp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.NaturalPG._fvp" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Build the Hessian-vector product.</p>
<p class="sd-card-text">Build the <a class="reference external" href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian-vector product</a> , which
is the second-order derivative of the KL-divergence.</p>
<p class="sd-card-text">The Hessian-vector product is approximated by the Fisher information matrix, which is the
second-order derivative of the KL-divergence.</p>
<p class="sd-card-text">For details see <a class="reference external" href="http://joschu.net/docs/thesis.pdf">John Schulman’s PhD thesis (pp. 40)</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><strong>params</strong> (<em>torch.Tensor</em>) – The parameters of the actor network.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><strong>The Fisher vector product.</strong></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.NaturalPG._init_log">
<span class="sig-name descname"><span class="pre">_init_log</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/natural_pg.html#NaturalPG._init_log"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.NaturalPG._init_log" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Log the Natural Policy Gradient specific information.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Things to log</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">Misc/AcceptanceStep</p></td>
<td><p class="sd-card-text">The acceptance step size.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Misc/Alpha</p></td>
<td><p class="sd-card-text"><span class="math notranslate nohighlight">\(\frac{\delta_{KL}}{xHx}\)</span> in the original paper.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Misc/FinalStepNorm</p></td>
<td><p class="sd-card-text">The final step norm.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Misc/gradient_norm</p></td>
<td><p class="sd-card-text">The gradient norm.</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Misc/xHx</p></td>
<td><p class="sd-card-text"><span class="math notranslate nohighlight">\(x H x\)</span> in the original paper.</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Misc/H_inv_g</p></td>
<td><p class="sd-card-text"><span class="math notranslate nohighlight">\(H^{-1} g\)</span> in the original paper.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.NaturalPG._update">
<span class="sig-name descname"><span class="pre">_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/natural_pg.html#NaturalPG._update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.NaturalPG._update" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update actor, critic.
:rtype: <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p class="sd-card-text">Here are some differences between NPG and Policy Gradient (PG): In PG, the actor network
and the critic network are updated together. When the KL divergence between the old
policy, and the new policy is larger than a threshold, the update is rejected together.</p>
<p class="sd-card-text">In NPG, the actor network and the critic network are updated separately. When the KL
divergence between the old policy, and the new policy is larger than a threshold, the
update of the actor network is rejected, but the update of the critic network is still
accepted.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.NaturalPG._update_actor">
<span class="sig-name descname"><span class="pre">_update_actor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_c</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/natural_pg.html#NaturalPG._update_actor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.NaturalPG._update_actor" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update policy network.</p>
<p class="sd-card-text">Natural Policy Gradient (NPG) update policy network using the conjugate gradient algorithm,
following the steps:</p>
<ul class="simple">
<li><p class="sd-card-text">Calculate the gradient of the policy network,</p></li>
<li><p class="sd-card-text">Use the conjugate gradient algorithm to calculate the step direction.</p></li>
<li><p class="sd-card-text">Update the policy network by taking a step in the step direction.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The observation tensor.</p></li>
<li><p class="sd-card-text"><strong>act</strong> (<em>torch.Tensor</em>) – The action tensor.</p></li>
<li><p class="sd-card-text"><strong>logp</strong> (<em>torch.Tensor</em>) – The log probability of the action.</p></li>
<li><p class="sd-card-text"><strong>adv_r</strong> (<em>torch.Tensor</em>) – The reward advantage tensor.</p></li>
<li><p class="sd-card-text"><strong>adv_c</strong> (<em>torch.Tensor</em>) – The cost advantage tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p class="sd-card-text"><strong>AssertionError</strong> – If <span class="math notranslate nohighlight">\(x\)</span> is not finite.</p></li>
<li><p class="sd-card-text"><strong>AssertionError</strong> – If <span class="math notranslate nohighlight">\(x H x\)</span> is not positive.</p></li>
<li><p class="sd-card-text"><strong>AssertionError</strong> – If <span class="math notranslate nohighlight">\(\alpha\)</span> is not finite.</p></li>
</ul>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</section>
<section id="trust-region-policy-optimization">
<span id="trpoapi"></span><h2>Trust Region Policy Optimization<a class="headerlink" href="#trust-region-policy-optimization" title="Permalink to this heading">#</a></h2>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-outline-success sd-rounded-1 docutils">
<div class="sd-card-header sd-bg-success sd-text-white docutils">
<p class="sd-card-text">Documentation</p>
</div>
<div class="sd-card-body docutils">
<dl class="py class">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.TRPO">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnisafe.algorithms.on_policy.</span></span><span class="sig-name descname"><span class="pre">TRPO</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cfgs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/trpo.html#TRPO"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.TRPO" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">The Trust Region Policy Optimization (TRPO) algorithm.</p>
<div class="admonition-references admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p class="sd-card-text">Title: Trust Region Policy Optimization</p></li>
<li><p class="sd-card-text">Authors: John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel.</p></li>
<li><p class="sd-card-text">URL: <a class="reference external" href="https://arxiv.org/abs/1502.05477">TRPO</a></p></li>
</ul>
</div>
<p class="sd-card-text">Initialize an instance of algorithm.</p>
<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.TRPO._init_log">
<span class="sig-name descname"><span class="pre">_init_log</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/trpo.html#TRPO._init_log"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.TRPO._init_log" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Log the Trust Region Policy Optimization specific information.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Things to log</p></th>
<th class="head"><p class="sd-card-text">Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">Misc/AcceptanceStep</p></td>
<td><p class="sd-card-text">The acceptance step size.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.TRPO._search_step_size">
<span class="sig-name descname"><span class="pre">_search_step_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step_direction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p_dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_before</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/trpo.html#TRPO._search_step_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.TRPO._search_step_size" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">TRPO performs <a class="reference external" href="https://en.wikipedia.org/wiki/Line_search">line-search</a> until constraint satisfaction.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p class="sd-card-text">TRPO search around for a satisfied step of policy update to improve loss and reward performance. The search
is done by line-search, which is a way to find a step size that satisfies the constraint. The constraint is
the KL-divergence between the old policy and the new policy.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>step_dir</strong> (<em>torch.Tensor</em>) – The step direction.</p></li>
<li><p class="sd-card-text"><strong>g_flat</strong> (<em>torch.Tensor</em>) – The gradient of the policy.</p></li>
<li><p class="sd-card-text"><strong>p_dist</strong> (<em>torch.distributions.Distribution</em>) – The old policy distribution.</p></li>
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The observation.</p></li>
<li><p class="sd-card-text"><strong>act</strong> (<em>torch.Tensor</em>) – The action.</p></li>
<li><p class="sd-card-text"><strong>logp</strong> (<em>torch.Tensor</em>) – The log probability of the action.</p></li>
<li><p class="sd-card-text"><strong>adv</strong> (<em>torch.Tensor</em>) – The advantage.</p></li>
<li><p class="sd-card-text"><strong>adv_c</strong> (<em>torch.Tensor</em>) – The cost advantage.</p></li>
<li><p class="sd-card-text"><strong>loss_pi_before</strong> (<em>float</em>) – The loss of the policy before the update.</p></li>
<li><p class="sd-card-text"><strong>total_steps</strong> (<em>int</em><em>, </em><em>optional</em>) – The total steps to search. Defaults to 15.</p></li>
<li><p class="sd-card-text"><strong>decay</strong> (<em>float</em><em>, </em><em>optional</em>) – The decay rate of the step size. Defaults to 0.8.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><strong>The tuple of final update direction and acceptance step size.</strong></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.TRPO._update_actor">
<span class="sig-name descname"><span class="pre">_update_actor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_c</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/trpo.html#TRPO._update_actor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.TRPO._update_actor" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Update policy network.</p>
<p class="sd-card-text">Trust Policy Region Optimization updates policy network using the
<a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate gradient</a> algorithm,
following the steps:</p>
<ul class="simple">
<li><p class="sd-card-text">Compute the gradient of the policy.</p></li>
<li><p class="sd-card-text">Compute the step direction.</p></li>
<li><p class="sd-card-text">Search for a step size that satisfies the constraint.</p></li>
<li><p class="sd-card-text">Update the policy network.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The observation tensor.</p></li>
<li><p class="sd-card-text"><strong>act</strong> (<em>torch.Tensor</em>) – The action tensor.</p></li>
<li><p class="sd-card-text"><strong>logp</strong> (<em>torch.Tensor</em>) – The log probability of the action.</p></li>
<li><p class="sd-card-text"><strong>adv_r</strong> (<em>torch.Tensor</em>) – The reward advantage tensor.</p></li>
<li><p class="sd-card-text"><strong>adv_c</strong> (<em>torch.Tensor</em>) – The cost advantage tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</section>
<section id="proximal-policy-optimization">
<span id="ppoapi"></span><h2>Proximal Policy Optimization<a class="headerlink" href="#proximal-policy-optimization" title="Permalink to this heading">#</a></h2>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-outline-success sd-rounded-1 docutils">
<div class="sd-card-header sd-bg-success sd-text-white docutils">
<p class="sd-card-text">Documentation</p>
</div>
<div class="sd-card-body docutils">
<dl class="py class">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PPO">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnisafe.algorithms.on_policy.</span></span><span class="sig-name descname"><span class="pre">PPO</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cfgs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/ppo.html#PPO"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PPO" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">The Proximal Policy Optimization (PPO) algorithm.</p>
<div class="admonition-references admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p class="sd-card-text">Title: Proximal Policy Optimization Algorithms</p></li>
<li><p class="sd-card-text">Authors: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov.</p></li>
<li><p class="sd-card-text">URL: <a class="reference external" href="https://arxiv.org/abs/1707.06347">PPO</a></p></li>
</ul>
</div>
<p class="sd-card-text">Initialize an instance of algorithm.</p>
<dl class="py method">
<dt class="sig sig-object py" id="omnisafe.algorithms.on_policy.PPO._loss_pi">
<span class="sig-name descname"><span class="pre">_loss_pi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/omnisafe/algorithms/on_policy/base/ppo.html#PPO._loss_pi"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#omnisafe.algorithms.on_policy.PPO._loss_pi" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Computing pi/actor loss.</p>
<p class="sd-card-text">In Proximal Policy Optimization, the loss is defined as:</p>
<div class="math-wrapper docutils container" id="equation-baserlapi-on-policy-7">
<div class="math notranslate nohighlight" id="equation-baserlapi-on-policy-7">
<span class="eqno">(8)<a class="headerlink" href="#equation-baserlapi-on-policy-7" title="Permalink to this equation">#</a></span>\[L^{CLIP} = \underset{s_t \sim \rho_{\theta}}{\mathbb{E}} \left[
    \min ( r_t A^{R}_{\pi_{\theta}} (s_t, a_t) , \text{clip} (r_t, 1 - \epsilon, 1 + \epsilon)
    A^{R}_{\pi_{\theta}} (s_t, a_t)
\right]\]</div>
</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(r_t = \frac{\pi_{\theta}^{'} (a_t|s_t)}{\pi_{\theta} (a_t|s_t)}\)</span>,
<span class="math notranslate nohighlight">\(\epsilon\)</span> is the clip parameter, and <span class="math notranslate nohighlight">\(A^{R}_{\pi_{\theta}} (s_t, a_t)\)</span> is the
advantage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>obs</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">observation</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>act</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">action</span></code> sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>logp</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">log</span> <span class="pre">probability</span></code> of action sampled from buffer.</p></li>
<li><p class="sd-card-text"><strong>adv</strong> (<em>torch.Tensor</em>) – The <code class="docutils literal notranslate"><span class="pre">advantage</span></code> processed. <code class="docutils literal notranslate"><span class="pre">reward_advantage</span></code> here.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p class="sd-card-text"><strong>The loss of pi/actor.</strong></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="off_policy.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Base Off-policy Algorithms</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../saferl/lag.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Lagrangian Methods</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022, OmniSafe Team
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link" href="https://readthedocs.org/projects/omnisafe" aria-label="On Read the Docs">
                <svg x="0px" y="0px" viewBox="-125 217 360 360" xml:space="preserve">
                  <path fill="currentColor" d="M39.2,391.3c-4.2,0.6-7.1,4.4-6.5,8.5c0.4,3,2.6,5.5,5.5,6.3 c0,0,18.5,6.1,50,8.7c25.3,2.1,54-1.8,54-1.8c4.2-0.1,7.5-3.6,7.4-7.8c-0.1-4.2-3.6-7.5-7.8-7.4c-0.5,0-1,0.1-1.5,0.2 c0,0-28.1,3.5-50.9,1.6c-30.1-2.4-46.5-7.9-46.5-7.9C41.7,391.3,40.4,391.1,39.2,391.3z M39.2,353.6c-4.2,0.6-7.1,4.4-6.5,8.5 c0.4,3,2.6,5.5,5.5,6.3c0,0,18.5,6.1,50,8.7c25.3,2.1,54-1.8,54-1.8c4.2-0.1,7.5-3.6,7.4-7.8c-0.1-4.2-3.6-7.5-7.8-7.4 c-0.5,0-1,0.1-1.5,0.2c0,0-28.1,3.5-50.9,1.6c-30.1-2.4-46.5-7.9-46.5-7.9C41.7,353.6,40.4,353.4,39.2,353.6z M39.2,315.9 c-4.2,0.6-7.1,4.4-6.5,8.5c0.4,3,2.6,5.5,5.5,6.3c0,0,18.5,6.1,50,8.7c25.3,2.1,54-1.8,54-1.8c4.2-0.1,7.5-3.6,7.4-7.8 c-0.1-4.2-3.6-7.5-7.8-7.4c-0.5,0-1,0.1-1.5,0.2c0,0-28.1,3.5-50.9,1.6c-30.1-2.4-46.5-7.9-46.5-7.9 C41.7,315.9,40.4,315.8,39.2,315.9z M39.2,278.3c-4.2,0.6-7.1,4.4-6.5,8.5c0.4,3,2.6,5.5,5.5,6.3c0,0,18.5,6.1,50,8.7 c25.3,2.1,54-1.8,54-1.8c4.2-0.1,7.5-3.6,7.4-7.8c-0.1-4.2-3.6-7.5-7.8-7.4c-0.5,0-1,0.1-1.5,0.2c0,0-28.1,3.5-50.9,1.6 c-30.1-2.4-46.5-7.9-46.5-7.9C41.7,278.2,40.4,278.1,39.2,278.3z M-13.6,238.5c-39.6,0.3-54.3,12.5-54.3,12.5v295.7 c0,0,14.4-12.4,60.8-10.5s55.9,18.2,112.9,19.3s71.3-8.8,71.3-8.8l0.8-301.4c0,0-25.6,7.3-75.6,7.7c-49.9,0.4-61.9-12.7-107.7-14.2 C-8.2,238.6-10.9,238.5-13.6,238.5z M19.5,257.8c0,0,24,7.9,68.3,10.1c37.5,1.9,75-3.7,75-3.7v267.9c0,0-19,10-66.5,6.6 C59.5,536.1,19,522.1,19,522.1L19.5,257.8z M-3.6,264.8c4.2,0,7.7,3.4,7.7,7.7c0,4.2-3.4,7.7-7.7,7.7c0,0-12.4,0.1-20,0.8 c-12.7,1.3-21.4,5.9-21.4,5.9c-3.7,2-8.4,0.5-10.3-3.2c-2-3.7-0.5-8.4,3.2-10.3c0,0,0,0,0,0c0,0,11.3-6,27-7.5 C-16,264.9-3.6,264.8-3.6,264.8z M-11,302.6c4.2-0.1,7.4,0,7.4,0c4.2,0.5,7.2,4.3,6.7,8.5c-0.4,3.5-3.2,6.3-6.7,6.7 c0,0-12.4,0.1-20,0.8c-12.7,1.3-21.4,5.9-21.4,5.9c-3.7,2-8.4,0.5-10.3-3.2c-2-3.7-0.5-8.4,3.2-10.3c0,0,11.3-6,27-7.5 C-20.5,302.9-15.2,302.7-11,302.6z M-3.6,340.2c4.2,0,7.7,3.4,7.7,7.7s-3.4,7.7-7.7,7.7c0,0-12.4-0.1-20,0.7 c-12.7,1.3-21.4,5.9-21.4,5.9c-3.7,2-8.4,0.5-10.3-3.2c-2-3.7-0.5-8.4,3.2-10.3c0,0,11.3-6,27-7.5C-16,340.1-3.6,340.2-3.6,340.2z" />
                </svg>
              </a>
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Base On-policy Algorithms</a><ul>
<li><a class="reference internal" href="#policy-gradient">Policy Gradient</a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient"><code class="docutils literal notranslate"><span class="pre">PolicyGradient</span></code></a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._compute_adv_surrogate"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._compute_adv_surrogate()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._init"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._init()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._init_env"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._init_env()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._init_log"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._init_log()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._init_model"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._init_model()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._loss_pi"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._loss_pi()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._update()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_actor"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._update_actor()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_cost_critic"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._update_cost_critic()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient._update_reward_critic"><code class="docutils literal notranslate"><span class="pre">PolicyGradient._update_reward_critic()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PolicyGradient.learn"><code class="docutils literal notranslate"><span class="pre">PolicyGradient.learn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#natural-policy-gradient">Natural Policy Gradient</a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.NaturalPG"><code class="docutils literal notranslate"><span class="pre">NaturalPG</span></code></a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.NaturalPG._fvp"><code class="docutils literal notranslate"><span class="pre">NaturalPG._fvp()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.NaturalPG._init_log"><code class="docutils literal notranslate"><span class="pre">NaturalPG._init_log()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.NaturalPG._update"><code class="docutils literal notranslate"><span class="pre">NaturalPG._update()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.NaturalPG._update_actor"><code class="docutils literal notranslate"><span class="pre">NaturalPG._update_actor()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#trust-region-policy-optimization">Trust Region Policy Optimization</a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.TRPO"><code class="docutils literal notranslate"><span class="pre">TRPO</span></code></a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.TRPO._init_log"><code class="docutils literal notranslate"><span class="pre">TRPO._init_log()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.TRPO._search_step_size"><code class="docutils literal notranslate"><span class="pre">TRPO._search_step_size()</span></code></a></li>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.TRPO._update_actor"><code class="docutils literal notranslate"><span class="pre">TRPO._update_actor()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#proximal-policy-optimization">Proximal Policy Optimization</a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PPO"><code class="docutils literal notranslate"><span class="pre">PPO</span></code></a><ul>
<li><a class="reference internal" href="#omnisafe.algorithms.on_policy.PPO._loss_pi"><code class="docutils literal notranslate"><span class="pre">PPO._loss_pi()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=dfa113cf"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="/_/static/javascript/readthedocs-doc-embed.js"></script>
    </body>
</html>